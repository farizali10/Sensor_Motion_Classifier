{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Set the warnings to be ignored\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def output_filler(df, path):\n",
    "    # Split the path using the appropriate separator for the current OS\n",
    "    path = os.path.normpath(path)\n",
    "    path_parts = path.split(os.path.sep)\n",
    "\n",
    "    # Get the second-to-last part of the path\n",
    "    output = path_parts[-2]\n",
    "\n",
    "    # Add a new \"Output\" column to the DataFrame and set it to the output value\n",
    "    df[\"Output\"] = output\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Task: bending1</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Frequency (Hz): 20</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># Clock (millisecond): 250</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># Duration (seconds): 120</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Columns: time,avg_rss12,var_rss12,avg_rss13,...</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0,39.25,0.43,22.75,0.43,33.75,1.30</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>118750,43.33,0.47,25.00,0.00,30.00,0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>119000,43.50,0.50,25.50,0.50,30.00,0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>119250,43.50,0.50,24.75,0.43,30.00,0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>119500,43.50,0.50,24.33,0.47,30.00,0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>119750,43.50,0.50,24.25,0.43,30.00,0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      # Task: bending1    Output\n",
       "0                                 # Frequency (Hz): 20  bending1\n",
       "1                           # Clock (millisecond): 250  bending1\n",
       "2                            # Duration (seconds): 120  bending1\n",
       "3    # Columns: time,avg_rss12,var_rss12,avg_rss13,...  bending1\n",
       "4                   0,39.25,0.43,22.75,0.43,33.75,1.30  bending1\n",
       "..                                                 ...       ...\n",
       "479            118750,43.33,0.47,25.00,0.00,30.00,0.00  bending1\n",
       "480            119000,43.50,0.50,25.50,0.50,30.00,0.00  bending1\n",
       "481            119250,43.50,0.50,24.75,0.43,30.00,0.00  bending1\n",
       "482            119500,43.50,0.50,24.33,0.47,30.00,0.00  bending1\n",
       "483            119750,43.50,0.50,24.25,0.43,30.00,0.00  bending1\n",
       "\n",
       "[484 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How output_filler works\n",
    "path = \"D:/Data-Science-D-drive/Datasets-D-drive/sensor_placement/bending1/dataset1.csv\"\n",
    "df = pd.read_csv(\"D:/Data-Science-D-drive/Datasets-D-drive/sensor_placement/bending1/dataset1.csv\", sep = \"\\t\")\n",
    "output_filler(df,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Output Column and Creating New DatFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging module\n",
    "# Remove or disable existing logging handlers\n",
    "root_logger = logging.getLogger()\n",
    "for handler in root_logger.handlers:\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "# Configure the logging module with a time format\n",
    "log_filename = \"error.log\"\n",
    "log_level = logging.DEBUG\n",
    "logging.basicConfig(filename=log_filename, level=log_level, format='%(asctime)s [%(levelname)s] %(message)s')  # Save errors+info to a file\n",
    "\n",
    "op_filepath_list = []\n",
    "        \n",
    "try:\n",
    "    logging.info(f\"Reading Main Folder\")\n",
    "    main_folder_path = \"D:\\Data-Science-D-drive\\Datasets-D-drive\\sensor_placement\"\n",
    "    main_folder_path = os.path.normpath(main_folder_path)\n",
    "    \n",
    "    try:\n",
    "        ##### LISTING AND STORING SUB-FOLDERS CONTENTS PRESENT IN MAIN FOLDER PATH I.E. BENDING1, BENDING2, SITTING, ETC ... #####\n",
    "        contents = os.listdir(main_folder_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing contents in {main_folder_path}: {e}\")\n",
    "\n",
    "    else:\n",
    "        for paths in contents:\n",
    "            ##### STORING SUB-FOLDERS PATHS PRESENT IN MAIN FOLDER PATH #####\n",
    "            sub_folder_path = os.path.join(main_folder_path, paths)\n",
    "\n",
    "            try:\n",
    "                #### LISTING CONTENTS PRESENT IN SUB-FOLDERS PATH I.E. DATASET1, DATASET2, DATASET3, ETC... ####\n",
    "                sub_folder_contents = os.listdir(sub_folder_path)\n",
    "                    \n",
    "                # CREATING \"OUTPUT\" DIRECTORY IN EACH SUB-FOLDERS TO STORE OUTPUT CSV #   \n",
    "                directory_name = \"output\"\n",
    "\n",
    "                output_path = os.path.join(sub_folder_path, directory_name)\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error listing contents in {sub_folder_path}: {e}\")\n",
    "\n",
    "            else:\n",
    "                path_list = []\n",
    "\n",
    "                for item in sub_folder_contents:\n",
    "                    #### STORING FILE'S PATH PRESENT IN SUB-FOLDERS I.E. DATASET1, DATASET2, DATASET3, ETC... ####\n",
    "                    file_path = os.path.join(sub_folder_path, item)\n",
    "                    path_list.append(file_path)\n",
    "                            \n",
    "                ### READING FILE PATHS IN SUB-FOLDERS ###\n",
    "                for path in path_list:\n",
    "                    logging.info(\"=\"*100)\n",
    "                    try:\n",
    "                        csv_content = pd.read_csv(path, sep=\",\", skiprows=4, header=0)\n",
    "                        csv_content = csv_content.reset_index(drop=True)\n",
    "                        csv_content = output_filler(csv_content, path)\n",
    "\n",
    "                        logging.info(f\"Processing {path}\")\n",
    "                        logging.info(csv_content.head(2).to_string(index=False))\n",
    "\n",
    "                        ### STORING NEW FILE'S CREATED IN OUTPUT FOLDER I.E. NEW_DATASET1, NEW_DATASET2, NEW_DATASET3, ETC... ####\n",
    "                        try:\n",
    "                            file_name = os.path.split(path)\n",
    "                            file_name = file_name[-1].split(\".\")[0]\n",
    "                        \n",
    "\n",
    "                            logging.info(\"Creating Output File Path\")\n",
    "                            output_file_path = os.path.join(output_path, f\"new_{file_name}.csv\")\n",
    "                            op_filepath_list.append(output_file_path)\n",
    "                            #csv_content.to_csv(output_file_path)\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error creating Output File Path for {file_name}: {e}\")\n",
    "\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error Reading contents in {path}: {e}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating New DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENDING1\n",
      "new_dataset1.csv Shape = (480, 9)\n",
      "new_dataset2.csv Shape = (480, 9)\n",
      "new_dataset3.csv Shape = (480, 9)\n",
      "new_dataset4.csv Shape = (480, 9)\n",
      "new_dataset5.csv Shape = (480, 9)\n",
      "new_dataset6.csv Shape = (480, 9)\n",
      "new_dataset7.csv Shape = (480, 9)\n",
      "BENDING2\n",
      "new_dataset1.csv Shape = (480, 9)\n",
      "new_dataset2.csv Shape = (480, 9)\n",
      "new_dataset3.csv Shape = (480, 9)\n",
      "new_dataset4.csv Shape = (480, 9)\n",
      "new_dataset5.csv Shape = (480, 9)\n",
      "new_dataset6.csv Shape = (480, 9)\n",
      "BENDINGTYPE.PDF\n",
      "CYCLING\n",
      "new_dataset1.csv Shape = (480, 9)\n",
      "new_dataset10.csv Shape = (480, 9)\n",
      "new_dataset11.csv Shape = (480, 9)\n",
      "new_dataset12.csv Shape = (480, 9)\n",
      "new_dataset13.csv Shape = (480, 9)\n",
      "new_dataset15.csv Shape = (480, 9)\n",
      "new_dataset2.csv Shape = (480, 9)\n",
      "new_dataset3.csv Shape = (480, 9)\n",
      "new_dataset4.csv Shape = (480, 9)\n",
      "new_dataset5.csv Shape = (480, 9)\n",
      "new_dataset6.csv Shape = (480, 9)\n",
      "new_dataset7.csv Shape = (480, 9)\n",
      "new_dataset8.csv Shape = (480, 9)\n",
      "LYING\n",
      "new_dataset1.csv Shape = (480, 9)\n",
      "new_dataset10.csv Shape = (480, 9)\n",
      "new_dataset11.csv Shape = (480, 9)\n",
      "new_dataset12.csv Shape = (480, 9)\n",
      "new_dataset13.csv Shape = (480, 9)\n",
      "new_dataset14.csv Shape = (480, 9)\n",
      "new_dataset15.csv Shape = (480, 9)\n",
      "new_dataset2.csv Shape = (480, 9)\n",
      "new_dataset3.csv Shape = (480, 9)\n",
      "new_dataset4.csv Shape = (480, 9)\n",
      "new_dataset5.csv Shape = (480, 9)\n",
      "new_dataset6.csv Shape = (480, 9)\n",
      "new_dataset7.csv Shape = (480, 9)\n",
      "new_dataset8.csv Shape = (480, 9)\n",
      "new_dataset9.csv Shape = (480, 9)\n",
      "SENSORSPLACEMENT.PDF\n",
      "SITTING\n",
      "new_dataset10.csv Shape = (480, 9)\n",
      "new_dataset11.csv Shape = (480, 9)\n",
      "new_dataset12.csv Shape = (480, 9)\n",
      "new_dataset13.csv Shape = (480, 9)\n",
      "new_dataset14.csv Shape = (480, 9)\n",
      "new_dataset15.csv Shape = (480, 9)\n",
      "new_dataset2.csv Shape = (480, 9)\n",
      "new_dataset3.csv Shape = (480, 9)\n",
      "new_dataset4.csv Shape = (480, 9)\n",
      "new_dataset5.csv Shape = (480, 9)\n",
      "new_dataset6.csv Shape = (480, 9)\n",
      "new_dataset7.csv Shape = (480, 9)\n",
      "new_dataset8.csv Shape = (479, 9)\n",
      "new_dataset9.csv Shape = (480, 9)\n",
      "STANDING\n",
      "new_dataset1.csv Shape = (480, 9)\n",
      "new_dataset10.csv Shape = (480, 9)\n",
      "new_dataset11.csv Shape = (480, 9)\n",
      "new_dataset12.csv Shape = (480, 9)\n",
      "new_dataset13.csv Shape = (480, 9)\n",
      "new_dataset14.csv Shape = (480, 9)\n",
      "new_dataset15.csv Shape = (480, 9)\n",
      "new_dataset2.csv Shape = (480, 9)\n",
      "new_dataset3.csv Shape = (480, 9)\n",
      "new_dataset4.csv Shape = (480, 9)\n",
      "new_dataset5.csv Shape = (480, 9)\n",
      "new_dataset6.csv Shape = (480, 9)\n",
      "new_dataset7.csv Shape = (480, 9)\n",
      "new_dataset8.csv Shape = (480, 9)\n",
      "new_dataset9.csv Shape = (480, 9)\n",
      "WALKING\n",
      "new_dataset1.csv Shape = (480, 9)\n",
      "new_dataset10.csv Shape = (480, 9)\n",
      "new_dataset11.csv Shape = (480, 9)\n",
      "new_dataset12.csv Shape = (480, 9)\n",
      "new_dataset13.csv Shape = (480, 9)\n",
      "new_dataset14.csv Shape = (480, 9)\n",
      "new_dataset15.csv Shape = (480, 9)\n",
      "new_dataset2.csv Shape = (480, 9)\n",
      "new_dataset3.csv Shape = (480, 9)\n",
      "new_dataset4.csv Shape = (480, 9)\n",
      "new_dataset5.csv Shape = (480, 9)\n",
      "new_dataset6.csv Shape = (480, 9)\n",
      "new_dataset7.csv Shape = (480, 9)\n",
      "new_dataset8.csv Shape = (480, 9)\n",
      "new_dataset9.csv Shape = (480, 9)\n"
     ]
    }
   ],
   "source": [
    "# Remove or disable existing logging handlers\n",
    "root_logger = logging.getLogger()\n",
    "for handler in root_logger.handlers:\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "# Configure the logging module with a time format\n",
    "log_filename = \"concatenation.log\"\n",
    "log_level = logging.DEBUG\n",
    "logging.basicConfig(filename=log_filename, level=log_level, format=\"%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)s] %(message)s\")\n",
    "\n",
    "try:\n",
    "    main_folder_path = \"D:\\Data-Science-D-drive\\Datasets-D-drive\\sensor_placement\"\n",
    "    main_folder_path = os.path.normpath(main_folder_path)\n",
    "    logging.info(f\"Reading Main Folder: {main_folder_path.split(os.path.sep)[-1].upper()}\")\n",
    "\n",
    "except Exception as e:\n",
    "        logging.error(f\"Error processing MAIN PATH {main_folder_path}: {e}\")\n",
    "\n",
    "# LISTING AND STORING SUB-FOLDERS CONTENTS PRESENT IN MAIN FOLDER PATH I.E. BENDING1, BENDING2, SITTING, ETC ...    \n",
    "try:\n",
    "    main_folder_contents = os.listdir(main_folder_path)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error listing MAIN_FOLDER_CONTENTS of MAIN PATH {main_folder_path}: {e}\")\n",
    "\n",
    "# STORING SUB-FOLDERS PATHS PRESENT IN MAIN FOLDER PATH\n",
    "try:\n",
    "    sub_folder_path_list = []\n",
    "\n",
    "    for item in main_folder_contents:\n",
    "        \n",
    "        sub_folder_path = os.path.join(main_folder_path, item)\n",
    "        sub_folder_path_list.append(sub_folder_path)\n",
    "\n",
    "except Exception as e:\n",
    "        logging.error(f\"Error Storing SUB_FOLDER_PATH {path}: {e}\")\n",
    "\n",
    "# LIST ALL FILES AND DIRECTORIES IN THE SUB_FOLDER\n",
    "\n",
    "for sub_path in sub_folder_path_list:\n",
    "    print(sub_path.split(os.path.sep)[-1].upper())\n",
    "    logging.info(f\"Listing FILE_CONTENTS of SUB_FOLDER_PATH: {sub_path.split(os.path.sep)[-1].upper()}\")\n",
    "    logging.info(\"=\"*100)     \n",
    "\n",
    "    try:\n",
    "        file_contents = os.listdir(sub_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error Listing FILE_CONTENTS of SUB_FOLDER_PATH {sub_path}: {e}\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            logging.info(\"Storing FILE_CONTENTS in SUB_FOLDER_PATH\")\n",
    "\n",
    "            files_path_list = []\n",
    "\n",
    "            # Store the paths\n",
    "            for item in file_contents:\n",
    "                file_path = os.path.join(sub_path, item)\n",
    "                files_path_list.append(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "                logging.error(f\"Error Storing FILE_CONTENTS in SUB_FOLDER_PATH {path}\")\n",
    "\n",
    "        else:\n",
    "            # Selecting output folder in sub_folder_files\n",
    "            try:\n",
    "                logging.info(\"Selecting output folder in sub_folder_files\")\n",
    "\n",
    "                for f_path in files_path_list:\n",
    "                    if \"output\" in f_path:\n",
    "                        logging.info(\"Storing OUTPUT_CONTENTS in OUTPUT_PATH\")\n",
    "\n",
    "                        # Store all files and directories in the output folder\n",
    "                        output_path = f_path \n",
    "                        output_contents = os.listdir(output_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                    logging.error(f\"Error Storing OUTPUT_CONTENTS in OUTPUT_PATH {path}: {e}\")\n",
    "\n",
    "            else:        \n",
    "                    try:\n",
    "                        logging.info(\"Storing OUTPUT_FILE_PATH in OP_FILEPATH_LIST\")              \n",
    "\n",
    "                        op_filepath_list = []\n",
    "\n",
    "                        # Store the paths\n",
    "                        for item in output_contents:\n",
    "                            output_file_path = os.path.join(output_path,item)\n",
    "                            op_filepath_list.append(output_file_path)\n",
    "\n",
    "                    except Exception as e:\n",
    "                            logging.error(f\"Error Storing OUTPUT_FILE_PATH in OP_FILEPATH_LIST {path}: {e}\")\n",
    "\n",
    "                    try: \n",
    "                        logging.info(\"Concatenating OUTPUT_FILEPATH of OP_FILEPATH_LIST\")\n",
    "\n",
    "                        final_df = pd.DataFrame()\n",
    "\n",
    "                        for output_filepath in op_filepath_list:\n",
    "                            df = pd.read_csv(output_filepath)\n",
    "                            df = df.reset_index(drop=True)\n",
    "\n",
    "                            final_df = pd.concat([final_df,df], ignore_index=True)\n",
    "                            \n",
    "                            # Get the second-to-last part of the path\n",
    "                            path_parts = output_filepath.split(os.path.sep)\n",
    "                            output_fname = path_parts[-1] \n",
    "                            print(f\"{output_fname} Shape = {df.shape}\")\n",
    "\n",
    "                        final_df_path = os.path.join(output_path, \"final_df.csv\")\n",
    "\n",
    "                        # Extract the first column as a DataFrame using iloc\n",
    "                        first_column_df = final_df.iloc[:, [0]]\n",
    "                        final_df = final_df.drop(first_column_df, axis=1)\n",
    "\n",
    "                        #final_df.to_csv(final_df_path)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error Concatenating OUTPUT_FILEPATH of OP_FILEPATH_LIST {path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating Sub-Folders Final Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENDING1\n",
      "final_df.csv Shape = (3360, 9)\n",
      "BENDING2\n",
      "final_df.csv Shape = (2880, 9)\n",
      "BENDINGTYPE.PDF\n",
      "CYCLING\n",
      "final_df.csv Shape = (6240, 9)\n",
      "LYING\n",
      "final_df.csv Shape = (7200, 9)\n",
      "SENSORSPLACEMENT.PDF\n",
      "SITTING\n",
      "final_df.csv Shape = (6719, 9)\n",
      "STANDING\n",
      "final_df.csv Shape = (7200, 9)\n",
      "WALKING\n",
      "final_df.csv Shape = (7200, 9)\n"
     ]
    }
   ],
   "source": [
    "# REMOVE OR DISABLE EXISTING LOGGING HANDLERS\n",
    "root_logger = logging.getLogger()\n",
    "for handler in root_logger.handlers:\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "# CONFIGURE THE LOGGING MODULE WITH A TIME FORMAT:\n",
    "log_filename = \"concatenation_final_df.log\"\n",
    "log_level = logging.DEBUG\n",
    "logging.basicConfig(filename=log_filename, level=log_level, format=\"%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)s] %(message)s\")\n",
    "\n",
    "try:\n",
    "    main_folder_path = \"D:\\Data-Science-D-drive\\Datasets-D-drive\\sensor_placement\"\n",
    "    main_folder_path = os.path.normpath(main_folder_path)\n",
    "    logging.info(f\"Reading Main Folder: {main_folder_path.split(os.path.sep)[-1].upper()}\")\n",
    "\n",
    "except Exception as e:\n",
    "        logging.error(f\"Error processing MAIN PATH {main_folder_path}: {e}\")\n",
    "\n",
    "# LISTING AND STORING SUB-FOLDERS CONTENTS PRESENT IN MAIN FOLDER PATH I.E. BENDING1, BENDING2, SITTING, ETC ...    \n",
    "try:\n",
    "    main_folder_contents = os.listdir(main_folder_path)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error listing MAIN_FOLDER_CONTENTS of MAIN PATH {main_folder_path}: {e}\")\n",
    "\n",
    "# STORING SUB-FOLDERS PATHS PRESENT IN MAIN FOLDER PATH\n",
    "try:\n",
    "    sub_folder_path_list = []\n",
    "\n",
    "    for item in main_folder_contents:\n",
    "        \n",
    "        sub_folder_path = os.path.join(main_folder_path, item)\n",
    "        sub_folder_path_list.append(sub_folder_path)\n",
    "\n",
    "except Exception as e:\n",
    "        logging.error(f\"Error Storing SUB_FOLDER_PATH {path}: {e}\")\n",
    "\n",
    "# LIST ALL FILES AND DIRECTORIES IN THE SUB_FOLDER\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "for sub_path in sub_folder_path_list:\n",
    "    print(sub_path.split(os.path.sep)[-1].upper())\n",
    "    logging.info(f\"Listing FILE_CONTENTS of SUB_FOLDER_PATH: {sub_path.split(os.path.sep)[-1].upper()}\")\n",
    "    logging.info(\"=\"*100)     \n",
    "\n",
    "    try:\n",
    "        file_contents = os.listdir(sub_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error Listing FILE_CONTENTS of SUB_FOLDER_PATH {sub_path}: {e}\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            logging.info(\"Storing FILE_CONTENTS in SUB_FOLDER_PATH\")\n",
    "\n",
    "            files_path_list = []\n",
    "\n",
    "            # Store the paths\n",
    "            for item in file_contents:\n",
    "                file_path = os.path.join(sub_path, item)\n",
    "                files_path_list.append(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "                logging.error(f\"Error Storing FILE_CONTENTS in SUB_FOLDER_PATH {path}\")\n",
    "\n",
    "        else:\n",
    "            # Selecting output folder in sub_folder_files\n",
    "            try:\n",
    "                logging.info(\"Selecting output folder in sub_folder_files\")\n",
    "\n",
    "                for f_path in files_path_list:\n",
    "                    if \"output\" in f_path:\n",
    "                        logging.info(\"Storing OUTPUT_CONTENTS in OUTPUT_PATH\")\n",
    "\n",
    "                        # Store all files and directories in the output folder\n",
    "                        output_path = f_path \n",
    "                        output_contents = os.listdir(output_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                    logging.error(f\"Error Storing OUTPUT_CONTENTS in OUTPUT_PATH {path}: {e}\")\n",
    "\n",
    "            else:        \n",
    "                    try:\n",
    "                        logging.info(\"Storing OUTPUT_FILE_PATH in OP_FILEPATH_LIST\")              \n",
    "\n",
    "                        op_filepath_list = []\n",
    "\n",
    "                        # Store the paths\n",
    "                        for item in output_contents:\n",
    "                            output_file_path = os.path.join(output_path,item)\n",
    "                            op_filepath_list.append(output_file_path)\n",
    "\n",
    "                    except Exception as e:\n",
    "                            logging.error(f\"Error Storing OUTPUT_FILE_PATH in OP_FILEPATH_LIST {path}: {e}\")\n",
    "######################################################\n",
    "                    try: \n",
    "                        logging.info(\"Selecting and Storing final_df.csv from OP_FILEPATH_LIST\")\n",
    "\n",
    "                        for output_filepath in op_filepath_list:                \n",
    "                            if \"final_df\" in output_filepath:\n",
    "                                df = pd.read_csv(output_filepath)\n",
    "                                df = df.reset_index(drop=True)\n",
    "\n",
    "                                output_df = pd.concat([output_df,df], ignore_index=True)\n",
    "                                \n",
    "                                # Get the second-to-last part of the path\n",
    "                                path_parts = output_filepath.split(os.path.sep)\n",
    "                                output_fname = path_parts[-1] \n",
    "                                print(f\"{output_fname} Shape = {df.shape}\")                                \n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error Concatenating OUTPUT_FILEPATH of OP_FILEPATH_LIST {output_filepath}: {e}\")\n",
    "try:\n",
    "    logging.info(\"Creating a path + Saving Output DF\")\n",
    "    output_df_path = os.path.join(main_folder_path, \"output_df.csv\")\n",
    "\n",
    "    # Extract the first column as a DataFrame using iloc\n",
    "    first_column_df = output_df.iloc[:, [0]]\n",
    "    output_df = output_df.drop(first_column_df, axis=1)\n",
    "\n",
    "    #output_df.to_csv(output_df_path)\n",
    "\n",
    "except Exception as e:\n",
    "     logging.error(\"Error while saving Output DF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Output DF's Classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bending1', 'bending2', 'cycling', 'lying', 'sitting', 'standing',\n",
       "       'walking'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[\"Output\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
